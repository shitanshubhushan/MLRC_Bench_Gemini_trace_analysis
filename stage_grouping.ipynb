{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GROUP BY STEPS TAKEN TO FIX A ERROR\n",
    "#### PASS TO O3 AND GET BETTER GROUPING LIKE DEBUGGING\n",
    "Maybe add how many steps it took to fix the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_agent_file(folder_path):\n",
    "    # Find all agent files in the folder\n",
    "    agent_files = glob.glob(os.path.join(folder_path, \"agent_*_*.json\"))\n",
    "    \n",
    "    if not agent_files:\n",
    "        return None\n",
    "    \n",
    "    # Extract agent numbers and find the highest one\n",
    "    max_agent_num = 0\n",
    "    max_agent_file = None\n",
    "    \n",
    "    for file in agent_files:\n",
    "        # Extract the number from filename using regex\n",
    "        match = re.search(r\"agent_(\\d+)_\\d+\\.json\", file)\n",
    "        if match:\n",
    "            agent_num = int(match.group(1))\n",
    "            if agent_num > max_agent_num:\n",
    "                max_agent_num = agent_num\n",
    "                max_agent_file = file\n",
    "    \n",
    "    return max_agent_file\n",
    "\n",
    "def extract_history_steps(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Extract history_steps without observation field\n",
    "        history_steps = data.get('history_steps', [])\n",
    "        \n",
    "        # Remove observation field from each step\n",
    "        for step in history_steps:\n",
    "            if 'observation' in step:\n",
    "                del step['observation']\n",
    "\n",
    "            if 'action' in step and isinstance(step['action'], dict):\n",
    "                if 'Research Plan and Status' in step['action']:\n",
    "                    del step['action']['Research Plan and Status']\n",
    "                if 'Fact Check' in step['action']:\n",
    "                    del step['action']['Fact Check']\n",
    "                \n",
    "        return history_steps\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_task_name_and_id(folder_path):\n",
    "    # Split the path into components\n",
    "    parts = folder_path.split(os.sep)\n",
    "    \n",
    "    # Look for task name and run ID in the path\n",
    "    task_name = None\n",
    "    run_id = None\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        if i < len(parts) - 2 and parts[i+2].startswith('0'):  # Assuming run IDs start with numbers\n",
    "            task_name = part\n",
    "            run_id = parts[i+2]\n",
    "            break\n",
    "    \n",
    "    return task_name, run_id\n",
    "\n",
    "def process_all_folders(base_path, output_base):\n",
    "    # Walk through directory structure\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Check if this folder contains agent files\n",
    "        agent_files = [f for f in files if f.startswith(\"agent_\") and f.endswith(\".json\")]\n",
    "        if agent_files:\n",
    "            last_agent = get_last_agent_file(root)\n",
    "            if last_agent:\n",
    "                history = extract_history_steps(last_agent)\n",
    "                if history:\n",
    "                    # Extract task name and run ID from the folder path\n",
    "                    task_name, run_id = extract_task_name_and_id(root)\n",
    "                    \n",
    "                    if task_name and run_id:\n",
    "                        # Create output directory\n",
    "                        output_dir = os.path.join(output_base, task_name, run_id)\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Save the extracted history\n",
    "                        output_file = os.path.join(output_dir, \"output.json\")\n",
    "                        with open(output_file, 'w') as f:\n",
    "                            json.dump(history, f, indent=2)\n",
    "                        \n",
    "                        print(f\"Extracted history from {root} saved to {output_file}\")\n",
    "                    else:\n",
    "                        print(f\"Could not determine task name and run ID for {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224204752_3547146/agent_log saved to Steps/product-recommendation/0224204752_3547146/output.json\n",
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224185433_2509699/agent_log saved to Steps/product-recommendation/0224185433_2509699/output.json\n",
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224204536_2561078/agent_log saved to Steps/product-recommendation/0224204536_2561078/output.json\n",
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224185433_896695/agent_log saved to Steps/product-recommendation/0224185433_896695/output.json\n",
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224185429_3486840/agent_log saved to Steps/product-recommendation/0224185429_3486840/output.json\n",
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224194239_3510088/agent_log saved to Steps/product-recommendation/0224194239_3510088/output.json\n",
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224214547_3576113/agent_log saved to Steps/product-recommendation/0224214547_3576113/output.json\n",
      "Extracted history from data/product-recommendation/gemini-exp-1206/0224193743_2527744/agent_log saved to Steps/product-recommendation/0224193743_2527744/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203215346_271668/agent_log saved to Steps/machine_unlearning/0203215346_271668/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203230341_2207563/agent_log saved to Steps/machine_unlearning/0203230341_2207563/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203223634_294909/agent_log saved to Steps/machine_unlearning/0203223634_294909/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203221257_2196521/agent_log saved to Steps/machine_unlearning/0203221257_2196521/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203205623_252068/agent_log saved to Steps/machine_unlearning/0203205623_252068/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203205627_2179792/agent_log saved to Steps/machine_unlearning/0203205627_2179792/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203205627_931199/agent_log saved to Steps/machine_unlearning/0203205627_931199/output.json\n",
      "Extracted history from data/machine_unlearning/gemini-exp-1206/0203214633_945041/agent_log saved to Steps/machine_unlearning/0203214633_945041/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0302041327_1728148/agent_log saved to Steps/meta-learning/0302041327_1728148/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0301222543_1509149/agent_log saved to Steps/meta-learning/0301222543_1509149/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0301233149_1550773/agent_log saved to Steps/meta-learning/0301233149_1550773/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0301222543_1028680/agent_log saved to Steps/meta-learning/0301222543_1028680/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0301231758_1111943/agent_log saved to Steps/meta-learning/0301231758_1111943/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0302003643_1592395/agent_log saved to Steps/meta-learning/0302003643_1592395/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0302033446_1263484/agent_log saved to Steps/meta-learning/0302033446_1263484/output.json\n",
      "Extracted history from data/meta-learning/gemini-exp-1206/0302032628_1192961/agent_log saved to Steps/meta-learning/0302032628_1192961/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0122093712/agent_log saved to Steps/llm-merging/0122093712/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0122064409/agent_log saved to Steps/llm-merging/0122064409/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0125132342/agent_log saved to Steps/llm-merging/0125132342/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0124003147/agent_log saved to Steps/llm-merging/0124003147/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0121011813/agent_log saved to Steps/llm-merging/0121011813/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0121150130/agent_log saved to Steps/llm-merging/0121150130/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0121155350/agent_log saved to Steps/llm-merging/0121155350/output.json\n",
      "Extracted history from data/llm-merging/gemini-exp-1206/0120235347/agent_log saved to Steps/llm-merging/0120235347/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0122112225/agent_log saved to Steps/backdoor-trigger-recovery/0122112225/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0122184351/agent_log saved to Steps/backdoor-trigger-recovery/0122184351/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0124053910/agent_log saved to Steps/backdoor-trigger-recovery/0124053910/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0123215426/agent_log saved to Steps/backdoor-trigger-recovery/0123215426/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0121132527/agent_log saved to Steps/backdoor-trigger-recovery/0121132527/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0121054359/agent_log saved to Steps/backdoor-trigger-recovery/0121054359/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0121184650/agent_log saved to Steps/backdoor-trigger-recovery/0121184650/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0122051552/agent_log saved to Steps/backdoor-trigger-recovery/0122051552/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/gemini-exp-1206/0120221803/agent_log saved to Steps/backdoor-trigger-recovery/0120221803/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130063132/agent_log saved to Steps/perception_temporal_action_loc/0130063132/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130052448/agent_log saved to Steps/perception_temporal_action_loc/0130052448/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130021741/agent_log saved to Steps/perception_temporal_action_loc/0130021741/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130103625/agent_log saved to Steps/perception_temporal_action_loc/0130103625/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130074053/agent_log saved to Steps/perception_temporal_action_loc/0130074053/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130102918/agent_log saved to Steps/perception_temporal_action_loc/0130102918/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130032056/agent_log saved to Steps/perception_temporal_action_loc/0130032056/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/gemini-exp-1206/0130182939/agent_log saved to Steps/perception_temporal_action_loc/0130182939/output.json\n"
     ]
    }
   ],
   "source": [
    "base_directory = \"data\"\n",
    "output_directory = \"Steps\"\n",
    "\n",
    "# Create the base output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process all folders\n",
    "process_all_folders(base_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_analyze_files(steps_dir):\n",
    "    \n",
    "    class Step(BaseModel):\n",
    "        Step_ID: int\n",
    "        Stage: int\n",
    "\n",
    "    class MathReasoning(BaseModel):\n",
    "        steps: list[Step]\n",
    "    \n",
    "    results = {}\n",
    "    failed_folders = []\n",
    "    retry_stats = []\n",
    "    max_retries = 5  # Maximum number of retries before giving up\n",
    "    \n",
    "    # Walk through the Steps directory\n",
    "    for task_name in os.listdir(steps_dir):\n",
    "        task_dir = os.path.join(steps_dir, task_name)\n",
    "        if not os.path.isdir(task_dir):\n",
    "            continue\n",
    "            \n",
    "        results[task_name] = {}\n",
    "        \n",
    "        for run_id in os.listdir(task_dir):\n",
    "            run_dir = os.path.join(task_dir, run_id)\n",
    "            if not os.path.isdir(run_dir):\n",
    "                continue\n",
    "                \n",
    "            output_file = os.path.join(run_dir, \"output.json\")\n",
    "            if not os.path.exists(output_file):\n",
    "                print(f\"No output.json found in {run_dir}\")\n",
    "                continue\n",
    "                \n",
    "            # Read the output.json file\n",
    "            try:\n",
    "                with open(output_file, 'r') as f:\n",
    "                    output_data = json.load(f)\n",
    "                \n",
    "                # Skip empty files or invalid data\n",
    "                if not output_data or not isinstance(output_data, list) or len(output_data) == 0:\n",
    "                    print(f\"Empty or invalid data in {run_dir}/output.json\")\n",
    "                    failed_folders.append(f\"{task_name}/{run_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Store the original step count\n",
    "                original_step_count = len(output_data)\n",
    "                \n",
    "                # Convert output_data to pretty-printed string for the prompt\n",
    "                output_json_str = json.dumps(output_data, indent=2)\n",
    "                \n",
    "                # Create prompt with the json content and explicitly mention step count\n",
    "                prompt = f\"\"\"\n",
    "                You are a researcher. You are given the following trace of an AI agent working on ML research challenges:\n",
    "\n",
    "                {output_json_str}\n",
    "\n",
    "                Your task is to analyze every step in the trace and assign a stage to each step. Use the following 7 stages. For each stage, use the reasoning guidelines provided to decide if a step belongs to that stage.\n",
    "\n",
    "                1. Understanding & Exploration:\n",
    "                - Description: Investigate the problem statement, explore the codebase, review data files, and understand evaluation metrics. This stage is about gathering context and building a solid grasp of the task and environment.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it focuses on examining available resources, reading documentation or files, exploring the code structure, or otherwise building an initial understanding of the project.\n",
    "\n",
    "                2. Baseline Assessment:\n",
    "                - Description: Evaluate the unmodified baseline solution's performance to collect performance metrics and establish a reference benchmark.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it focuses on measuring the performance of the original, unaltered solution, collecting data for baseline comparison, and ensuring the initial performance level is documented. Do not assign a step to this stage if it executes the solution after changes have been made.\n",
    "\n",
    "                3. Problem Analysis & Idea Generation:\n",
    "                - Description: Analyze the baseline results to identify shortcomings and brainstorm potential improvements or alternative strategies.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it is centered on evaluating baseline outcomes, identifying issues, or generating ideas and strategies for potential improvements.\n",
    "\n",
    "                4. Implementation:\n",
    "                - Description: Develop and integrate the proposed modifications into the codebase by editing, extending, or refactoring the existing solution.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it involves writing new code, modifying existing code, or integrating changes aimed at improving the solution.\n",
    "\n",
    "                5. Debugging & Error Handling:\n",
    "                - Description: Identify, isolate, and fix any errors or unexpected behaviors introduced during implementation to ensure the solution runs reliably.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it is focused on diagnosing problems, investigating error messages, or making corrections to ensure proper functionality.\n",
    "\n",
    "                6. Experimental Refinement:\n",
    "                - Description: Re-run experiments on an already implemented solution and iteratively test various configurations, tune parameters, and compare alternative approaches to upgrade performance.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it involves re-executing or adjusting an implemented solution, making upgrades and modifications to improve performance after the initial implementation has been established.\n",
    "\n",
    "                7. Final Evaluation & Submission:\n",
    "                - Description: Conduct a comprehensive evaluation of the refined solution against benchmarks and prepare the solution for final submission.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it involves performing a final, thorough evaluation of the solution’s performance, verifying that all improvements meet the required criteria, and preparing for submission.\n",
    "\n",
    "                Your response must be a JSON object where the keys are the step numbers (as strings) and the values are the corresponding stage numbers (from 1 to 7) that best describe the agent's activity at that step.\n",
    "\n",
    "                IMPORTANT: When assigning a stage, review the steps before and after each step to understand the broader context.\n",
    "\n",
    "                IMPORTANT: The original trace has {original_step_count} steps. Your response MUST contain exactly {original_step_count} keys, numbered from \"1\" to \"{original_step_count}\".\n",
    "\n",
    "                Example output format:\n",
    "                {{\n",
    "                \"1\": 1,\n",
    "                \"2\": 1,\n",
    "                \"3\": 4,\n",
    "                \"4\": 6,\n",
    "                \"5\": 7,\n",
    "                \"6\": 7,\n",
    "                ...\n",
    "                }}\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"Processing {task_name}/{run_id}...\")\n",
    "                \n",
    "                # Retry loop for handling step count mismatches\n",
    "                retry_count = 0\n",
    "                success = False\n",
    "                \n",
    "                while not success and retry_count < max_retries:\n",
    "                    try:\n",
    "                        # Call the API\n",
    "                        completion = client.beta.chat.completions.parse(\n",
    "                            model=\"gpt-4o\",\n",
    "                            messages=[\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": prompt\n",
    "                                }\n",
    "                            ],\n",
    "                            response_format=MathReasoning\n",
    "                        )\n",
    "                        \n",
    "                        # Extract the response\n",
    "                        response_content = completion.choices[0].message.content\n",
    "                        \n",
    "                        # Parse the response\n",
    "                        response_json = json.loads(response_content)\n",
    "                        \n",
    "                        # Validate step count\n",
    "                        if len(response_json['steps']) != original_step_count:\n",
    "                            retry_count += 1\n",
    "                            print(f\"Step count mismatch in {task_name}/{run_id} (attempt {retry_count}/{max_retries}): Original has {original_step_count} steps, but analysis has {len(response_json)} steps\")\n",
    "                            \n",
    "                            # Add more explicit instructions for the retry\n",
    "                            prompt = f\"\"\"\n",
    "                            You are a researcher, given the following trace of an AI agent doing ML research challenges:\n",
    "                            {output_json_str}\n",
    "                            \n",
    "                            Your task is to analyze every step in the trace and assign a stage to each step. Use the following 7 stages:\n",
    "\n",
    "                1. Understanding & Exploration:\n",
    "                - Description: Investigate the problem statement, explore the codebase, review data files, and understand evaluation metrics. This stage is about gathering context and building a solid grasp of the task and environment.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it focuses on examining available resources, reading documentation or files, exploring the code structure, or otherwise building an initial understanding of the project.\n",
    "\n",
    "                2. Baseline Assessment:\n",
    "                - Description: Evaluate the unmodified baseline solution's performance to collect performance metrics and establish a reference benchmark.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it focuses on measuring the performance of the original, unaltered solution, collecting data for baseline comparison, and ensuring the initial performance level is documented. Do not assign a step to this stage if it executes the solution after changes have been made.\n",
    "\n",
    "                3. Problem Analysis & Idea Generation:\n",
    "                - Description: Analyze the baseline results to identify shortcomings and brainstorm potential improvements or alternative strategies.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it is centered on evaluating baseline outcomes, identifying issues, or generating ideas and strategies for potential improvements.\n",
    "\n",
    "                4. Implementation:\n",
    "                - Description: Develop and integrate the proposed modifications into the codebase by editing, extending, or refactoring the existing solution.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it involves writing new code, modifying existing code, or integrating changes aimed at improving the solution.\n",
    "\n",
    "                5. Debugging & Error Handling:\n",
    "                - Description: Identify, isolate, and fix any errors or unexpected behaviors introduced during implementation to ensure the solution runs reliably.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it is focused on diagnosing problems, investigating error messages, or making corrections to ensure proper functionality.\n",
    "\n",
    "                6. Experimental Refinement:\n",
    "                - Description: Re-run experiments on an already implemented solution and iteratively test various configurations, tune parameters, and compare alternative approaches to upgrade performance.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it involves re-executing or adjusting an implemented solution, making upgrades and modifications to improve performance after the initial implementation has been established.\n",
    "\n",
    "                7. Final Evaluation & Submission:\n",
    "                - Description: Conduct a comprehensive evaluation of the refined solution against benchmarks and prepare the solution for final submission.\n",
    "                - Reasoning Guideline: Assign a step to this stage if it involves performing a final, thorough evaluation of the solution’s performance, verifying that all improvements meet the required criteria, and preparing for submission.\n",
    "\n",
    "                            Your response must be a JSON object where the keys are the step numbers (as strings) and the values are the corresponding stage numbers (from 1 to 7) that best describe the agent's activity at that step.\n",
    "                            \n",
    "                            IMPORTANT: When assigning a stage, also review the steps before and after to understand the broader context.\n",
    "                            \n",
    "                            CRITICAL ERROR IN PREVIOUS ATTEMPT: You provided {len(response_json)} steps, but the original trace has EXACTLY {original_step_count} steps.\n",
    "                            \n",
    "                            Your response MUST have EXACTLY {original_step_count} steps, with keys from \"1\" to \"{original_step_count}\".\n",
    "                            Make sure to include every step number from 1 to {original_step_count} in your response.\n",
    "                            \n",
    "                            Example format (if there were 12 steps):\n",
    "                            {{\n",
    "                              \"1\": 1,\n",
    "                              \"2\": 2,\n",
    "                              \"3\": 3,\n",
    "                              \"4\": 4,\n",
    "                              \"5\": 5,\n",
    "                              \"6\": 5,\n",
    "                              \"7\": 5,\n",
    "                              \"8\": 5,\n",
    "                              \"9\": 5,\n",
    "                              \"10\": 5,\n",
    "                              \"11\": 5,\n",
    "                              \"12\": 5,\n",
    "                            }}\n",
    "                            \n",
    "                            But your response should have {original_step_count} steps, not 12.\n",
    "                            Analyze every step in the trace and ensure your response has EXACTLY {original_step_count} entries.\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            # Add a small delay before retrying\n",
    "                            time.sleep(2)\n",
    "                        else:\n",
    "                            # If step count matches, we're successful\n",
    "                            success = True\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        retry_count += 1\n",
    "                        print(f\"Error processing {task_name}/{run_id} (attempt {retry_count}/{max_retries}): {str(e)}\")\n",
    "                        time.sleep(2)\n",
    "                \n",
    "                # After the retry loop, check if we had success\n",
    "                if success:\n",
    "                    # Save the analysis result\n",
    "                    analysis_file = os.path.join(run_dir, \"analysis.json\")\n",
    "                    with open(analysis_file, 'w') as f:\n",
    "                        json.dump(response_json, f, indent=2)\n",
    "                        \n",
    "                    results[task_name][run_id] = response_json\n",
    "                    \n",
    "                    # Record retry stats if we needed retries\n",
    "                    if retry_count > 0:\n",
    "                        retry_stats.append(f\"{task_name}/{run_id}: Succeeded after {retry_count} retries\")\n",
    "                        \n",
    "                    print(f\"Analysis saved to {analysis_file}\" + (f\" after {retry_count} retries\" if retry_count > 0 else \"\"))\n",
    "                    \n",
    "                else:\n",
    "                    # All retries failed\n",
    "                    print(f\"Failed to get correct step count for {task_name}/{run_id} after {max_retries} attempts\")\n",
    "                    failed_folders.append(f\"{task_name}/{run_id}\")\n",
    "                    retry_stats.append(f\"{task_name}/{run_id}: Failed after {max_retries} retries\")\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting\n",
    "                time.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file for {task_name}/{run_id}: {str(e)}\")\n",
    "                failed_folders.append(f\"{task_name}/{run_id}\")\n",
    "    \n",
    "    # Save the compiled results\n",
    "    compiled_results_file = os.path.join(steps_dir, \"compiled_analysis.json\")\n",
    "    with open(compiled_results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save the list of failed folders\n",
    "    failed_folders_file = os.path.join(steps_dir, \"failed_folders.txt\")\n",
    "    with open(failed_folders_file, 'w') as f:\n",
    "        for folder in failed_folders:\n",
    "            f.write(f\"{folder}\\n\")\n",
    "    \n",
    "    # Save the retry statistics\n",
    "    retry_stats_file = os.path.join(steps_dir, \"retry_stats.txt\")\n",
    "    with open(retry_stats_file, 'w') as f:\n",
    "        for stat in retry_stats:\n",
    "            f.write(f\"{stat}\\n\")\n",
    "    \n",
    "    print(f\"Compiled results saved to {compiled_results_file}\")\n",
    "    print(f\"Failed folders list saved to {failed_folders_file}\")\n",
    "    print(f\"Retry statistics saved to {retry_stats_file}\")\n",
    "    print(f\"Total failed folders: {len(failed_folders)}\")\n",
    "    print(f\"Folders that needed retries: {len(retry_stats)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product-recommendation/0224204752_3547146...\n",
      "Analysis saved to Steps/product-recommendation/0224204752_3547146/analysis.json\n",
      "Processing product-recommendation/0224185433_2509699...\n",
      "Analysis saved to Steps/product-recommendation/0224185433_2509699/analysis.json\n",
      "Processing product-recommendation/0224204536_2561078...\n",
      "Analysis saved to Steps/product-recommendation/0224204536_2561078/analysis.json\n",
      "Processing product-recommendation/0224185433_896695...\n",
      "Analysis saved to Steps/product-recommendation/0224185433_896695/analysis.json\n",
      "Processing product-recommendation/0224185429_3486840...\n",
      "Analysis saved to Steps/product-recommendation/0224185429_3486840/analysis.json\n",
      "Processing product-recommendation/0224194239_3510088...\n",
      "Analysis saved to Steps/product-recommendation/0224194239_3510088/analysis.json\n",
      "Processing product-recommendation/0224214547_3576113...\n",
      "Analysis saved to Steps/product-recommendation/0224214547_3576113/analysis.json\n",
      "Processing product-recommendation/0224193743_2527744...\n",
      "Analysis saved to Steps/product-recommendation/0224193743_2527744/analysis.json\n",
      "Processing machine_unlearning/0203215346_271668...\n",
      "Analysis saved to Steps/machine_unlearning/0203215346_271668/analysis.json\n",
      "Processing machine_unlearning/0203230341_2207563...\n",
      "Analysis saved to Steps/machine_unlearning/0203230341_2207563/analysis.json\n",
      "Processing machine_unlearning/0203223634_294909...\n",
      "Analysis saved to Steps/machine_unlearning/0203223634_294909/analysis.json\n",
      "Processing machine_unlearning/0203221257_2196521...\n",
      "Analysis saved to Steps/machine_unlearning/0203221257_2196521/analysis.json\n",
      "Processing machine_unlearning/0203205623_252068...\n",
      "Analysis saved to Steps/machine_unlearning/0203205623_252068/analysis.json\n",
      "Processing machine_unlearning/0203205627_2179792...\n",
      "Analysis saved to Steps/machine_unlearning/0203205627_2179792/analysis.json\n",
      "Processing machine_unlearning/0203205627_931199...\n",
      "Analysis saved to Steps/machine_unlearning/0203205627_931199/analysis.json\n",
      "Processing machine_unlearning/0203214633_945041...\n",
      "Analysis saved to Steps/machine_unlearning/0203214633_945041/analysis.json\n",
      "Processing meta-learning/0302041327_1728148...\n",
      "Analysis saved to Steps/meta-learning/0302041327_1728148/analysis.json\n",
      "Processing meta-learning/0301222543_1509149...\n",
      "Analysis saved to Steps/meta-learning/0301222543_1509149/analysis.json\n",
      "Processing meta-learning/0301233149_1550773...\n",
      "Analysis saved to Steps/meta-learning/0301233149_1550773/analysis.json\n",
      "Processing meta-learning/0301222543_1028680...\n",
      "Analysis saved to Steps/meta-learning/0301222543_1028680/analysis.json\n",
      "Processing meta-learning/0301231758_1111943...\n",
      "Analysis saved to Steps/meta-learning/0301231758_1111943/analysis.json\n",
      "Processing meta-learning/0302003643_1592395...\n",
      "Analysis saved to Steps/meta-learning/0302003643_1592395/analysis.json\n",
      "Processing meta-learning/0302033446_1263484...\n",
      "Analysis saved to Steps/meta-learning/0302033446_1263484/analysis.json\n",
      "Processing meta-learning/0302032628_1192961...\n",
      "Analysis saved to Steps/meta-learning/0302032628_1192961/analysis.json\n",
      "Processing llm-merging/0122093712...\n",
      "Analysis saved to Steps/llm-merging/0122093712/analysis.json\n",
      "Processing llm-merging/0122064409...\n",
      "Analysis saved to Steps/llm-merging/0122064409/analysis.json\n",
      "Processing llm-merging/0125132342...\n",
      "Analysis saved to Steps/llm-merging/0125132342/analysis.json\n",
      "Processing llm-merging/0124003147...\n",
      "Analysis saved to Steps/llm-merging/0124003147/analysis.json\n",
      "Processing llm-merging/0121011813...\n",
      "Analysis saved to Steps/llm-merging/0121011813/analysis.json\n",
      "Processing llm-merging/0121150130...\n",
      "Analysis saved to Steps/llm-merging/0121150130/analysis.json\n",
      "Processing llm-merging/0121155350...\n",
      "Analysis saved to Steps/llm-merging/0121155350/analysis.json\n",
      "Processing llm-merging/0120235347...\n",
      "Analysis saved to Steps/llm-merging/0120235347/analysis.json\n",
      "Processing backdoor-trigger-recovery/0122112225...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0122112225/analysis.json\n",
      "Processing backdoor-trigger-recovery/0122184351...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0122184351/analysis.json\n",
      "Processing backdoor-trigger-recovery/0124053910...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0124053910/analysis.json\n",
      "Processing backdoor-trigger-recovery/0123215426...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0123215426/analysis.json\n",
      "Processing backdoor-trigger-recovery/0121132527...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0121132527/analysis.json\n",
      "Processing backdoor-trigger-recovery/0121054359...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0121054359/analysis.json\n",
      "Processing backdoor-trigger-recovery/0121184650...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0121184650/analysis.json\n",
      "Processing backdoor-trigger-recovery/0122051552...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0122051552/analysis.json\n",
      "Processing backdoor-trigger-recovery/0120221803...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0120221803/analysis.json\n",
      "Processing perception_temporal_action_loc/0130063132...\n",
      "Error processing perception_temporal_action_loc/0130063132 (attempt 1/5): Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Dc6smt4QHa643ceKJCSz5Bz6 on tokens per min (TPM): Limit 30000, Requested 76998. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error processing perception_temporal_action_loc/0130063132 (attempt 2/5): Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Dc6smt4QHa643ceKJCSz5Bz6 on tokens per min (TPM): Limit 30000, Requested 76998. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error processing perception_temporal_action_loc/0130063132 (attempt 3/5): Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Dc6smt4QHa643ceKJCSz5Bz6 on tokens per min (TPM): Limit 30000, Requested 76998. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error processing perception_temporal_action_loc/0130063132 (attempt 4/5): Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Dc6smt4QHa643ceKJCSz5Bz6 on tokens per min (TPM): Limit 30000, Requested 76998. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error processing perception_temporal_action_loc/0130063132 (attempt 5/5): Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Dc6smt4QHa643ceKJCSz5Bz6 on tokens per min (TPM): Limit 30000, Requested 76998. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Failed to get correct step count for perception_temporal_action_loc/0130063132 after 5 attempts\n",
      "Processing perception_temporal_action_loc/0130052448...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130052448/analysis.json\n",
      "Processing perception_temporal_action_loc/0130021741...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130021741/analysis.json\n",
      "Processing perception_temporal_action_loc/0130103625...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130103625/analysis.json\n",
      "Processing perception_temporal_action_loc/0130074053...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130074053/analysis.json\n",
      "Processing perception_temporal_action_loc/0130102918...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130102918/analysis.json\n",
      "Processing perception_temporal_action_loc/0130032056...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130032056/analysis.json\n",
      "Processing perception_temporal_action_loc/0130182939...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130182939/analysis.json\n",
      "Compiled results saved to Steps/compiled_analysis.json\n",
      "Failed folders list saved to Steps/failed_folders.txt\n",
      "Retry statistics saved to Steps/retry_stats.txt\n",
      "Total failed folders: 1\n",
      "Folders that needed retries: 1\n"
     ]
    }
   ],
   "source": [
    "# Process all the files\n",
    "results = process_and_analyze_files(\"Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing failed folders, If Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved to Steps/perception_temporal_action_loc/0130063132/output.json\n"
     ]
    }
   ],
   "source": [
    "problem_folder = \"Steps/perception_temporal_action_loc/0130063132\"\n",
    "input_file = os.path.join(problem_folder, \"output.json\")\n",
    "output_file = os.path.join(problem_folder, \"output.json\")\n",
    "\n",
    "# Read the original file\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Process each step to remove \"Action Input\"\n",
    "for step in data:\n",
    "    if \"action\" in step and \"Action Input\" in step[\"action\"]:\n",
    "        del step[\"action\"][\"Action Input\"]\n",
    "\n",
    "# Save the modified data\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"Modified data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Timestamop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timestamps_to_analysis(steps_dir, data_dir):\n",
    "    # Process all task directories\n",
    "    for task_name in os.listdir(steps_dir):\n",
    "        task_dir = os.path.join(steps_dir, task_name)\n",
    "        if not os.path.isdir(task_dir):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing task: {task_name}\")\n",
    "        \n",
    "        # For each run directory\n",
    "        for run_id in os.listdir(task_dir):\n",
    "            run_dir = os.path.join(task_dir, run_id)\n",
    "            if not os.path.isdir(run_dir):\n",
    "                continue\n",
    "                \n",
    "            # Find the analysis.json file\n",
    "            analysis_file = os.path.join(run_dir, \"analysis.json\")\n",
    "            if not os.path.exists(analysis_file):\n",
    "                print(f\"  No analysis.json found in {run_dir}\")\n",
    "                continue\n",
    "                \n",
    "            # Find the corresponding trace.json file\n",
    "            trace_file = find_trace_file(data_dir, task_name, run_id)\n",
    "            if not trace_file:\n",
    "                print(f\"  No trace.json found for {task_name}/{run_id}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Processing run: {run_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Load the analysis data\n",
    "                with open(analysis_file, 'r') as f:\n",
    "                    analysis_data = json.load(f)\n",
    "                \n",
    "                # Load the trace data\n",
    "                with open(trace_file, 'r') as f:\n",
    "                    trace_data = json.load(f)\n",
    "                \n",
    "                # Add timestamps to analysis steps\n",
    "                if 'steps' in analysis_data:\n",
    "                    updated_steps = add_timestamps(analysis_data['steps'], trace_data)\n",
    "                    analysis_data['steps'] = updated_steps\n",
    "                else:\n",
    "                    print(f\"  Warning: analysis.json doesn't have 'steps' key in {run_dir}\")\n",
    "                \n",
    "                # Save the updated analysis file\n",
    "                with open(analysis_file, 'w') as f:\n",
    "                    json.dump(analysis_data, f, indent=2)\n",
    "                    \n",
    "                print(f\"  Added timestamps to {analysis_file}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {run_id}: {str(e)}\")\n",
    "\n",
    "def find_trace_file(data_dir, task_name, run_id):\n",
    "    \"\"\"Find the trace.json file for the corresponding task/run_id\"\"\"\n",
    "    # Based on your directory structure from the screenshots\n",
    "    path_pattern = os.path.join(data_dir, task_name, \"*\", run_id, \"env_log\", \"trace.json\")\n",
    "    \n",
    "    # Use glob to find matching files\n",
    "    matching_files = glob.glob(path_pattern)\n",
    "    \n",
    "    if matching_files:\n",
    "        return matching_files[0]  # Return the first match\n",
    "    \n",
    "    # Try with a more flexible pattern if the first one fails\n",
    "    path_pattern = os.path.join(data_dir, task_name, \"**\", run_id, \"**\", \"trace.json\")\n",
    "    matching_files = glob.glob(path_pattern, recursive=True)\n",
    "    \n",
    "    if matching_files:\n",
    "        return matching_files[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def add_timestamps(analysis_steps, trace_data):\n",
    "    \"\"\"Add timestamps from trace_data to analysis_steps\"\"\"\n",
    "    # Create a mapping of step index to timestamp\n",
    "    timestamps = {}\n",
    "    \n",
    "    # Check if trace_data is a dictionary with 'steps' key\n",
    "    if isinstance(trace_data, dict) and 'steps' in trace_data:\n",
    "        trace_steps = trace_data['steps']\n",
    "    else:\n",
    "        # Assume trace_data is already the steps array\n",
    "        trace_steps = trace_data\n",
    "    \n",
    "    for i, step in enumerate(trace_steps, 1):  # Start index from 1\n",
    "        if 'timestamp' in step:\n",
    "            timestamps[i] = step['timestamp']\n",
    "    \n",
    "    # Add timestamps to analysis steps\n",
    "    for step in analysis_steps:\n",
    "        step_id = step['Step_ID']\n",
    "        if step_id in timestamps:\n",
    "            step['Timestamp'] = timestamps[step_id]\n",
    "        else:\n",
    "            print(f\"    Warning: No timestamp found for step {step_id}\")\n",
    "    \n",
    "    return analysis_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing task: erasing_invisible_watermarks\n",
      "  Processing run: 0324210310_289223\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0324210310_289223/analysis.json\n",
      "  Processing run: 0324190630_1638924\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0324190630_1638924/analysis.json\n",
      "  Processing run: 0325014937_1397017\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0325014937_1397017/analysis.json\n",
      "  Processing run: 0324190631_207567\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0324190631_207567/analysis.json\n",
      "  Processing run: 0325010735_1875048\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0325010735_1875048/analysis.json\n",
      "  Processing run: 0324233014_384607\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0324233014_384607/analysis.json\n",
      "  Processing run: 0324212142_1730023\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0324212142_1730023/analysis.json\n",
      "  Processing run: 0324232835_1813028\n",
      "  Added timestamps to New_Tasks_Steps/erasing_invisible_watermarks/0324232835_1813028/analysis.json\n",
      "Processing task: weather_forcast\n",
      "  Processing run: 0328143417_3772595\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0328143417_3772595/analysis.json\n",
      "  Processing run: 0328145927_2596962\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0328145927_2596962/analysis.json\n",
      "  Processing run: 0328145927_2596963\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0328145927_2596963/analysis.json\n",
      "  Processing run: 0327143510_3682554\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0327143510_3682554/analysis.json\n",
      "  Processing run: 0328143840_1129568\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0328143840_1129568/analysis.json\n",
      "  Processing run: 0328145927_2596961\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0328145927_2596961/analysis.json\n",
      "  Processing run: 0328143552_135814\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0328143552_135814/analysis.json\n",
      "  Processing run: 0327064119_3238650\n",
      "  Added timestamps to New_Tasks_Steps/weather_forcast/0327064119_3238650/analysis.json\n",
      "Timestamp addition complete!\n"
     ]
    }
   ],
   "source": [
    "steps_dir = \"Steps\"\n",
    "data_dir = \"data\" \n",
    "\n",
    "add_timestamps_to_analysis(steps_dir, data_dir)\n",
    "print(\"Timestamp addition complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To fix in case API returns string instead of numbers, Usually not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_stages_in_json(file_path):\n",
    "    # Stage mapping (text to numeric)\n",
    "    reverse_mapping = {\n",
    "        \"problem/starter code/idea understanding\": \"1\",\n",
    "        \"propose idea and implement\": \"2\",\n",
    "        \"execute\": \"3\",\n",
    "        \"reflect and improve\": \"4\",\n",
    "        \"submission\": \"5\"\n",
    "    }\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Check if 'steps' exists\n",
    "    if 'steps' not in data:\n",
    "        print(f\"Error: No 'steps' key found in {file_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Apply reverse mapping to convert text stages to numeric\n",
    "    changes_made = 0\n",
    "    for step in data['steps']:\n",
    "        if 'Stage' in step and step['Stage'] in reverse_mapping:\n",
    "            step['Stage'] = reverse_mapping[step['Stage']]\n",
    "            changes_made += 1\n",
    "    \n",
    "    # Save the updated JSON file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Fixed {changes_made} stages in {file_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix stages in analysis.json Only Run in case API returns string instead of numbers\n",
    "#file_path = \"Steps/machine_unlearning/0204002434_497948/analysis.json\"\n",
    "#fix_stages_in_json(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new tasks to main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define stage names for reference\n",
    "stage_names = {\n",
    "    1: \"understanding_exploration\",\n",
    "    2: \"baseline_assessment\",\n",
    "    3: \"problem_analysis_idea_generation\",\n",
    "    4: \"implementation\",\n",
    "    5: \"debugging_error_handling\",\n",
    "    6: \"experimental_refinement\",\n",
    "    7: \"final_evaluation_submission\"\n",
    "}\n",
    "\n",
    "def extract_metadata_from_path(file_path):\n",
    "    \"\"\"Extract task and run_id from file path\"\"\"\n",
    "    # Expected path format: Steps/task_name/run_id/analysis.json\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    \n",
    "    # Find steps index (might be \"Steps\" or lowercase \"steps\")\n",
    "    steps_index = -1\n",
    "    for i, part in enumerate(path_parts):\n",
    "        if part.lower() == \"steps\":\n",
    "            steps_index = i\n",
    "            break\n",
    "    \n",
    "    # Get task name (folder right after \"Steps\")\n",
    "    task = path_parts[steps_index + 1] if steps_index >= 0 and steps_index + 1 < len(path_parts) else \"unknown\"\n",
    "    \n",
    "    # The run_id is typically the folder containing the analysis.json file\n",
    "    run_id = path_parts[-2] if len(path_parts) >= 2 else \"unknown\"\n",
    "    \n",
    "    return task, run_id\n",
    "\n",
    "def process_stage_transitions(steps):\n",
    "    \"\"\"Process steps to identify stage transitions and durations\"\"\"\n",
    "    # Sort steps by ID to ensure chronological order\n",
    "    steps = sorted(steps, key=lambda x: x.get('Step_ID', 0))\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    transitions = []\n",
    "    current_stage = None\n",
    "    start_step = None\n",
    "    start_time = None\n",
    "    last_step_id = None\n",
    "    last_step_time = None\n",
    "    \n",
    "    for step in steps:\n",
    "        # Convert stage to int (fix: ensure stage is an int)\n",
    "        stage_str = step.get('Stage')\n",
    "        if stage_str is None or step.get('Step_ID') is None:\n",
    "            continue  # Skip steps with missing data\n",
    "            \n",
    "        # Convert stage to int\n",
    "        try:\n",
    "            stage = int(stage_str)\n",
    "        except (ValueError, TypeError):\n",
    "            continue  # Skip steps with invalid stage values\n",
    "        \n",
    "        step_id = step.get('Step_ID')\n",
    "        step_time = step.get('Timestamp')\n",
    "        \n",
    "        # Keep track of the last step in the current stage\n",
    "        if stage == current_stage:\n",
    "            last_step_id = step_id\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # When stage changes, record the previous stage block\n",
    "        elif current_stage is not None:\n",
    "            # For stages with only a single step:\n",
    "            if start_step == last_step_id:\n",
    "                # If this is not the first step in the entire process\n",
    "                prev_step_time = None\n",
    "                for prev_step in steps:\n",
    "                    if prev_step['Step_ID'] == start_step - 1:\n",
    "                        prev_step_time = prev_step.get('Timestamp')\n",
    "                        break\n",
    "                \n",
    "                # If we found the previous step, use its timestamp as the start time\n",
    "                if prev_step_time is not None and last_step_time is not None:\n",
    "                    duration = last_step_time - prev_step_time\n",
    "                else:\n",
    "                    # Fallback: estimate based on the current step\n",
    "                    duration = 5  # default seconds for a single step with no context\n",
    "            else:\n",
    "                # For multi-step stages, calculate duration\n",
    "                duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "            \n",
    "            # Add the previous stage block\n",
    "            transitions.append({\n",
    "                'stage': current_stage,\n",
    "                'start_step': start_step,\n",
    "                'end_step': last_step_id,\n",
    "                'step_count': last_step_id - start_step + 1,\n",
    "                'start_time': start_time,\n",
    "                'end_time': last_step_time,\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            # Start a new stage block\n",
    "            current_stage = stage\n",
    "            start_step = step_id\n",
    "            \n",
    "            # Find the timestamp of the previous step to use as start_time\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Use previous step's timestamp as start_time\n",
    "            start_time = prev_step_time\n",
    "            \n",
    "            last_step_id = step_id\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # Initialize the first stage\n",
    "        else:\n",
    "            current_stage = stage\n",
    "            start_step = step_id\n",
    "            \n",
    "            # For the first step, use a reasonable time before\n",
    "            if step_time is not None:\n",
    "                # Estimate start time as 5 seconds before first timestamp\n",
    "                start_time = step_time - 5  # assuming the first step took 5 seconds\n",
    "            else:\n",
    "                start_time = None\n",
    "                \n",
    "            last_step_id = step_id\n",
    "            last_step_time = step_time\n",
    "    \n",
    "    # Add the final stage block\n",
    "    if current_stage is not None:\n",
    "        # For single step final stage\n",
    "        if start_step == last_step_id:\n",
    "            # Try to find the previous step timestamp\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Calculate duration from previous step to current\n",
    "            if prev_step_time is not None and last_step_time is not None:\n",
    "                duration = last_step_time - prev_step_time\n",
    "            else:\n",
    "                # If no previous step, estimate a reasonable duration\n",
    "                duration = 5  # default seconds\n",
    "        else:\n",
    "            # For multi-step stages\n",
    "            duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "        \n",
    "        transitions.append({\n",
    "            'stage': current_stage,\n",
    "            'start_step': start_step,\n",
    "            'end_step': last_step_id,\n",
    "            'step_count': last_step_id - start_step + 1,\n",
    "            'start_time': start_time,\n",
    "            'end_time': last_step_time,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "def calculate_stage_summaries(transitions):\n",
    "    \"\"\"Calculate summaries by stage from transitions data\"\"\"\n",
    "    stage_counts = {}\n",
    "    stage_durations = {}\n",
    "    \n",
    "    for t in transitions:\n",
    "        stage = t.get('stage')\n",
    "        if stage is None:\n",
    "            continue\n",
    "            \n",
    "        # Accumulate counts and durations (using int stage as key)\n",
    "        stage_counts[stage] = stage_counts.get(stage, 0) + t.get('step_count', 0)\n",
    "        if t.get('duration') is not None:\n",
    "            stage_durations[stage] = stage_durations.get(stage, 0) + t.get('duration')\n",
    "    \n",
    "    return stage_counts, stage_durations\n",
    "\n",
    "def process_analysis_file(file_path):\n",
    "    \"\"\"Process a single analysis.json file and return summary data\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract task and run_id from file path\n",
    "        task, run_id = extract_metadata_from_path(file_path)\n",
    "        \n",
    "        # Process the steps to get transitions\n",
    "        steps = data.get('steps', [])\n",
    "        transitions = process_stage_transitions(steps)\n",
    "        \n",
    "        # Calculate stage summaries\n",
    "        stage_counts, stage_durations = calculate_stage_summaries(transitions)\n",
    "        \n",
    "        # Create the summary object\n",
    "        summary = {\n",
    "            \"task\": task,\n",
    "            \"model\": \"claude\",  # Always claude as specified\n",
    "            \"run_id\": run_id\n",
    "        }\n",
    "        \n",
    "        # Add stage information (using int stages)\n",
    "        for stage in range(1, 8):\n",
    "            summary[f\"stage{stage}_time\"] = stage_durations.get(stage, 0)  # In seconds\n",
    "            summary[f\"stage{stage}_steps\"] = stage_counts.get(stage, 0)\n",
    "        \n",
    "        # Add total information\n",
    "        summary[\"total_time\"] = sum(duration for duration in stage_durations.values() if duration is not None)\n",
    "        summary[\"total_steps\"] = sum(stage_counts.values())\n",
    "        \n",
    "        # Add transition information for deeper analysis\n",
    "        summary[\"transitions\"] = transitions\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_all_analysis_files(root_dir):\n",
    "    \"\"\"Find all analysis.json files in the directory structure\"\"\"\n",
    "    return glob.glob(os.path.join(root_dir, \"**\", \"analysis.json\"), recursive=True)\n",
    "\n",
    "def create_summary_jsonl(summaries, output_file):\n",
    "    \"\"\"Create a JSONL file from the summaries, excluding the transitions field\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for summary in summaries:\n",
    "            # Create a copy without the transitions field for the JSONL output\n",
    "            summary_copy = {k: v for k, v in summary.items() if k != \"transitions\"}\n",
    "            f.write(json.dumps(summary_copy) + '\\n')\n",
    "\n",
    "def main(root_dir, output_file):\n",
    "    \"\"\"Process all analysis files and create JSONL output\"\"\"\n",
    "    # Find all analysis files\n",
    "    analysis_files = find_all_analysis_files(root_dir)\n",
    "    print(f\"Found {len(analysis_files)} analysis files\")\n",
    "    \n",
    "    # Process each file and collect summaries\n",
    "    summaries = []\n",
    "    for file_path in analysis_files:\n",
    "        print(f\"Processing {file_path}\")\n",
    "        summary = process_analysis_file(file_path)\n",
    "        if summary:\n",
    "            summaries.append(summary)\n",
    "    \n",
    "    # Write the summaries to JSONL file (without transitions field)\n",
    "    create_summary_jsonl(summaries, output_file)\n",
    "    print(f\"Wrote {len(summaries)} summaries to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 analysis files\n",
      "Processing Steps/erasing_invisible_watermarks/0324210310_289223/analysis.json\n",
      "Processing Steps/erasing_invisible_watermarks/0324190630_1638924/analysis.json\n",
      "Processing Steps/erasing_invisible_watermarks/0325014937_1397017/analysis.json\n",
      "Processing Steps/erasing_invisible_watermarks/0324190631_207567/analysis.json\n",
      "Processing Steps/erasing_invisible_watermarks/0325010735_1875048/analysis.json\n",
      "Processing Steps/erasing_invisible_watermarks/0324233014_384607/analysis.json\n",
      "Processing Steps/erasing_invisible_watermarks/0324212142_1730023/analysis.json\n",
      "Processing Steps/erasing_invisible_watermarks/0324232835_1813028/analysis.json\n",
      "Processing Steps/product-recommendation/0224185430_2454915/analysis.json\n",
      "Processing Steps/product-recommendation/0224234847_4180532/analysis.json\n",
      "Processing Steps/product-recommendation/0225001535_2648062/analysis.json\n",
      "Processing Steps/product-recommendation/0224185433_4037560/analysis.json\n",
      "Processing Steps/product-recommendation/0224221958_4139760/analysis.json\n",
      "Processing Steps/product-recommendation/0224185433_2354896/analysis.json\n",
      "Processing Steps/product-recommendation/0224211359_4108585/analysis.json\n",
      "Processing Steps/product-recommendation/0224202331_4083186/analysis.json\n",
      "Processing Steps/machine_unlearning/0203205627_279760/analysis.json\n",
      "Processing Steps/machine_unlearning/0203205627_2107738/analysis.json\n",
      "Processing Steps/machine_unlearning/0204004906_2156777/analysis.json\n",
      "Processing Steps/machine_unlearning/0204021048_1005508/analysis.json\n",
      "Processing Steps/machine_unlearning/0204000906_980483/analysis.json\n",
      "Processing Steps/machine_unlearning/0204002434_497948/analysis.json\n",
      "Processing Steps/machine_unlearning/0203235437_311468/analysis.json\n",
      "Processing Steps/machine_unlearning/0203205627_931197/analysis.json\n",
      "Processing Steps/meta-learning/0302045750_3278224/analysis.json\n",
      "Processing Steps/meta-learning/0301231854_3122029/analysis.json\n",
      "Processing Steps/meta-learning/0301222543_2494011/analysis.json\n",
      "Processing Steps/meta-learning/0301222543_3809111/analysis.json\n",
      "Processing Steps/meta-learning/0302053437_4014001/analysis.json\n",
      "Processing Steps/meta-learning/0302070548_2753844/analysis.json\n",
      "Processing Steps/meta-learning/0302032634_3952353/analysis.json\n",
      "Processing Steps/meta-learning/0302025323_2628250/analysis.json\n",
      "Processing Steps/llm-merging/0121081654/analysis.json\n",
      "Processing Steps/llm-merging/0124071448/analysis.json\n",
      "Processing Steps/llm-merging/0305091549/analysis.json\n",
      "Processing Steps/llm-merging/0124110854/analysis.json\n",
      "Processing Steps/llm-merging/0122132158/analysis.json\n",
      "Processing Steps/llm-merging/0121200118/analysis.json\n",
      "Processing Steps/llm-merging/0121073211/analysis.json\n",
      "Processing Steps/llm-merging/0121214229/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0124032241/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0121200721/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0124144709/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0122092147/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0121045246/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0122164524/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0121105812/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0122014648/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130104224/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130021740/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130045001/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130093111/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0307112653_4055421/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0307112653_4055420/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130102918/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130102836/analysis.json\n",
      "Wrote 56 summaries to stage_summaries.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT_DIR = \"Steps\"\n",
    "OUTPUT_FILE = \"stage_summaries.jsonl\"\n",
    "VIS_DIR = \"visualizations\"\n",
    "\n",
    "main(ROOT_DIR, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def process_analysis_json(file_path, output_dir):\n",
    "    \"\"\"\n",
    "    Process a single analysis.json file and create a timeline visualization.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the analysis.json file\n",
    "        output_dir: Directory to save the output figure\n",
    "    \"\"\"\n",
    "    # Extract task name from file path\n",
    "    # The path format is Steps/task_name/timestamp/analysis.json\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    task_name = path_parts[-3] if len(path_parts) >= 3 else \"Unknown_Task\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the JSON data\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'steps' not in data:\n",
    "        print(f\"Warning: {file_path} does not contain 'steps' key, skipping\")\n",
    "        return\n",
    "    \n",
    "    # Process the data\n",
    "    steps = data['steps']\n",
    "    \n",
    "    # Define stage names - updated to 7 stages\n",
    "    stage_names = {\n",
    "        1: \"understanding_exploration\",\n",
    "        2: \"baseline_assessment\",\n",
    "        3: \"problem_analysis_idea_generation\",\n",
    "        4: \"implementation\",\n",
    "        5: \"debugging_error_handling\",\n",
    "        6: \"experimental_refinement\",\n",
    "        7: \"final_evaluation_submission\"\n",
    "    }\n",
    "    \n",
    "    # Define stage colors - updated to 7 stages with distinct colors\n",
    "    stage_colors = {\n",
    "        1: \"#b3e6b3\",  # Light green\n",
    "        2: \"#b3b3e6\",  # Light blue\n",
    "        3: \"#e6b3b3\",  # Light red\n",
    "        4: \"#e6e6b3\",  # Light yellow\n",
    "        5: \"#e6b3e6\",  # Light purple\n",
    "        6: \"#b3e6e6\",  # Light cyan\n",
    "        7: \"#e6cc99\"   # Light orange\n",
    "    }\n",
    "    \n",
    "    # Group steps by stage transitions\n",
    "    transitions = []\n",
    "    current_stage = None\n",
    "    start_step = None\n",
    "    start_time = None\n",
    "    last_step_id = None\n",
    "    last_step_time = None\n",
    "    \n",
    "    for step in steps:\n",
    "        # Skip steps without Stage information\n",
    "        if 'Stage' not in step:\n",
    "            continue\n",
    "            \n",
    "        stage = step['Stage']\n",
    "        step_time = step.get('Timestamp')\n",
    "        \n",
    "        # Keep track of the last step in the current stage\n",
    "        if stage == current_stage:\n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # When stage changes, record the previous stage block\n",
    "        elif current_stage is not None:\n",
    "            # For stages with only a single step:\n",
    "            if start_step == last_step_id:\n",
    "                # If this is not the first step in the entire process\n",
    "                prev_step_time = None\n",
    "                for prev_step in steps:\n",
    "                    if prev_step['Step_ID'] == start_step - 1:\n",
    "                        prev_step_time = prev_step.get('Timestamp')\n",
    "                        break\n",
    "                \n",
    "                # If we found the previous step, use its timestamp as the start time\n",
    "                if prev_step_time is not None and last_step_time is not None:\n",
    "                    duration = last_step_time - prev_step_time\n",
    "                else:\n",
    "                    # Fallback: estimate based on the current step\n",
    "                    duration = 5  # default seconds for a single step with no context\n",
    "            else:\n",
    "                # For multi-step stages, the duration is from the prior stage's last step\n",
    "                # to this stage's last step\n",
    "                duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "            \n",
    "            # Add the previous stage block\n",
    "            transitions.append({\n",
    "                'stage': current_stage,\n",
    "                'start_step': start_step,\n",
    "                'end_step': last_step_id,\n",
    "                'position': len(transitions),\n",
    "                'start_time': start_time,\n",
    "                'end_time': last_step_time,\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            # Start a new stage block\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # Find the timestamp of the previous step to use as start_time\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Use previous step's timestamp as start_time\n",
    "            start_time = prev_step_time\n",
    "            \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # Initialize the first stage\n",
    "        else:\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # For the first step, use a reasonable time before\n",
    "            if step_time is not None:\n",
    "                # Estimate start time as 5 seconds before first timestamp\n",
    "                start_time = step_time - 5  # assuming the first step took 5 seconds\n",
    "            else:\n",
    "                start_time = None\n",
    "                \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "    \n",
    "    # Add the final stage block\n",
    "    if current_stage is not None:\n",
    "        # For single step final stage\n",
    "        if start_step == last_step_id:\n",
    "            # Try to find the previous step timestamp\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Calculate duration from previous step to current\n",
    "            if prev_step_time is not None and last_step_time is not None:\n",
    "                duration = last_step_time - prev_step_time\n",
    "            else:\n",
    "                # If no previous step, estimate a reasonable duration\n",
    "                duration = 5  # default seconds\n",
    "        else:\n",
    "            # For multi-step stages\n",
    "            duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "        \n",
    "        transitions.append({\n",
    "            'stage': current_stage,\n",
    "            'start_step': start_step,\n",
    "            'end_step': last_step_id,\n",
    "            'position': len(transitions),\n",
    "            'start_time': start_time,\n",
    "            'end_time': last_step_time,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    # Count steps in each stage for proportional sizing\n",
    "    for t in transitions:\n",
    "        t['step_count'] = t['end_step'] - t['start_step'] + 1\n",
    "    \n",
    "    # Create the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))  # Increased width and height for better legend readability\n",
    "    \n",
    "    # Setup the plot\n",
    "    y_pos = 0.6  # Move up to make room for time labels\n",
    "    height = 0.6\n",
    "    total_width = 1.0\n",
    "    \n",
    "    # Calculate total duration for scaling\n",
    "    total_duration = sum(t['duration'] for t in transitions if t['duration'] is not None)\n",
    "    if total_duration <= 0:\n",
    "        # Fallback to step-based sizing if durations are not available\n",
    "        total_steps = sum(t['step_count'] for t in transitions)\n",
    "        use_duration_scaling = False\n",
    "    else:\n",
    "        use_duration_scaling = True\n",
    "    \n",
    "    block_positions = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    # Draw the timeline blocks\n",
    "    min_block_width = 0.02  # Minimum width for visibility\n",
    "    adjusted_total_width = total_width - (min_block_width * len([t for t in transitions if t['duration'] is not None and (t['duration'] / total_duration) * total_width < min_block_width]))\n",
    "    \n",
    "    for i, block in enumerate(transitions):\n",
    "        # Calculate width proportional to duration instead of step count\n",
    "        if use_duration_scaling and block['duration'] is not None:\n",
    "            # Calculate the proportional width\n",
    "            proportional_width = (block['duration'] / total_duration) * total_width\n",
    "            \n",
    "            # Apply minimum width if needed\n",
    "            if proportional_width < min_block_width:\n",
    "                block_width = min_block_width\n",
    "            else:\n",
    "                # Scale the remaining width to account for minimum widths\n",
    "                adjustment_factor = adjusted_total_width / total_width if adjusted_total_width > 0 else 1\n",
    "                block_width = proportional_width * adjustment_factor\n",
    "        else:\n",
    "            # Fallback to step count if duration is not available\n",
    "            block_width = (block['step_count'] / total_steps) * total_width\n",
    "            # Apply minimum width\n",
    "            if block_width < min_block_width:\n",
    "                block_width = min_block_width\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (current_pos, y_pos - height/2),\n",
    "            block_width,\n",
    "            height,\n",
    "            linewidth=1,\n",
    "            edgecolor='black',\n",
    "            facecolor=stage_colors.get(block['stage'], \"#cccccc\"),  # Default to gray if stage not in colors\n",
    "            alpha=0.8\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add step count to each block\n",
    "        mid_point = current_pos + block_width/2\n",
    "        ax.text(\n",
    "            mid_point, y_pos, \n",
    "            f\"{block['step_count']}\", \n",
    "            ha='center', va='center', \n",
    "            fontsize=9, \n",
    "            fontweight='bold',\n",
    "            color='black'\n",
    "        )\n",
    "        \n",
    "        # Add time duration below each block\n",
    "        if block['duration'] is not None:\n",
    "            # Format duration (in seconds)\n",
    "            duration_seconds = block['duration']\n",
    "            if duration_seconds >= 3600:  # More than an hour\n",
    "                hours = int(duration_seconds // 3600)\n",
    "                minutes = int((duration_seconds % 3600) // 60)\n",
    "                duration_str = f\"{hours}h {minutes}m\"\n",
    "            elif duration_seconds >= 60:  # More than a minute\n",
    "                minutes = int(duration_seconds // 60)\n",
    "                seconds = int(duration_seconds % 60)\n",
    "                duration_str = f\"{minutes}m {seconds}s\"\n",
    "            else:\n",
    "                duration_str = f\"{int(duration_seconds)}s\"\n",
    "            \n",
    "            # Add time label with rotation for better fit\n",
    "            # Position it slightly below the block\n",
    "            ax.text(\n",
    "                mid_point, y_pos - height/2 - 0.1, \n",
    "                duration_str, \n",
    "                ha='center', va='top', \n",
    "                fontsize=8,\n",
    "                color='black',\n",
    "                rotation=90 if block_width < 0.05 else 0  # Rotate label if block is narrow\n",
    "            )\n",
    "        \n",
    "        # Store position for timeline\n",
    "        block_positions.append((current_pos, block_width))\n",
    "        \n",
    "        # Move to next position\n",
    "        current_pos += block_width\n",
    "    \n",
    "    # Draw timeline line - moved further down to avoid overlapping with labels\n",
    "    timeline_y_position = y_pos - height/2 - 0.35  # Increased this value to move line down\n",
    "    ax.plot([0, total_width], [timeline_y_position, timeline_y_position], 'k-', alpha=0.5)\n",
    "    \n",
    "    # Set up the axes\n",
    "    ax.set_xlim(-0.05, total_width + 0.1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add a legend for stages with full stage names\n",
    "    legend_elements = []\n",
    "    for stage in sorted(set(t['stage'] for t in transitions)):\n",
    "        if stage in stage_names:\n",
    "            # Format stage name for display (replace underscores with spaces and capitalize)\n",
    "            display_name = stage_names.get(stage, \"unknown\").replace('_', ' ').title()\n",
    "            \n",
    "            legend_elements.append(\n",
    "                patches.Patch(facecolor=stage_colors.get(stage, \"#cccccc\"), \n",
    "                              edgecolor='black', \n",
    "                              label=f'Stage {stage}: {display_name}')\n",
    "            )\n",
    "    \n",
    "    # Create a legend with 3 columns for better readability\n",
    "    plt.legend(handles=legend_elements, loc='upper center', \n",
    "               bbox_to_anchor=(0.5, -0.2), ncol=3, fontsize=9)\n",
    "    \n",
    "    # Format task name for display\n",
    "    display_task_name = task_name.replace('_', ' ').title()\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f'{display_task_name} Stage Timeline', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add context about scaling method\n",
    "    if use_duration_scaling:\n",
    "        plt.figtext(0.5, 0.01, \n",
    "                    \"Note: Block widths are proportional to time duration in each stage, with minimum size for visibility.\\nDuration times shown below each block.\\nNumber in each block is the number of steps in that stage.\", \n",
    "                    ha='center', fontsize=8)\n",
    "    else:\n",
    "        plt.figtext(0.5, 0.01, \n",
    "                    \"Note: Block widths are proportional to number of steps in each stage.\\nDuration times shown below each block.\", \n",
    "                    ha='center', fontsize=8)\n",
    "    \n",
    "    # Tight layout with more space at bottom for legend\n",
    "    plt.tight_layout(rect=[0, 0.2, 1, 0.95])\n",
    "    \n",
    "    # Generate output filename\n",
    "    output_filename = os.path.join(output_dir, f\"{task_name}_stage_timeline.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Generated timeline for {task_name} at {output_filename}\")\n",
    "\n",
    "def find_analysis_files(root_dir):\n",
    "    \"\"\"\n",
    "    Find all analysis.json files under the root directory\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory to start the search\n",
    "        \n",
    "    Returns:\n",
    "        List of paths to analysis.json files\n",
    "    \"\"\"\n",
    "    analysis_files = []\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'analysis.json' in filenames:\n",
    "            analysis_files.append(os.path.join(dirpath, 'analysis.json'))\n",
    "    \n",
    "    return analysis_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 analysis.json files to process\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0324210310_289223/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0324190630_1638924/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0325014937_1397017/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0324190631_207567/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0325010735_1875048/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0324233014_384607/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0324212142_1730023/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/0324232835_1813028/erasing_invisible_watermarks_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0224185430_2454915/product-recommendation_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0224234847_4180532/product-recommendation_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0225001535_2648062/product-recommendation_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0224185433_4037560/product-recommendation_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0224221958_4139760/product-recommendation_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0224185433_2354896/product-recommendation_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0224211359_4108585/product-recommendation_stage_timeline.png\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/0224202331_4083186/product-recommendation_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203205627_279760/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203205627_2107738/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204004906_2156777/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204021048_1005508/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204000906_980483/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204002434_497948/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203235437_311468/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203205627_931197/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0302045750_3278224/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0301231854_3122029/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0301222543_2494011/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0301222543_3809111/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0302053437_4014001/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0302070548_2753844/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0302032634_3952353/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0302025323_2628250/meta-learning_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121081654/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0124071448/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0305091549/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0124110854/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0122132158/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121200118/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121073211/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121214229/llm-merging_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0124032241/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0121200721/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0124144709/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0122092147/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0121045246/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0122164524/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0121105812/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0122014648/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130104224/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130021740/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130045001/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130093111/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0307112653_4055421/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0307112653_4055420/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130102918/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130102836/perception_temporal_action_loc_stage_timeline.png\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'Steps'\n",
    "\n",
    "# Find all analysis.json files\n",
    "analysis_files = find_analysis_files(root_dir)\n",
    "\n",
    "print(f\"Found {len(analysis_files)} analysis.json files to process\")\n",
    "\n",
    "# Process each file\n",
    "for file_path in analysis_files:\n",
    "    # Get the directory containing the analysis.json file\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    \n",
    "    # Process the file\n",
    "    process_analysis_json(file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Run Timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_analysis_json(file_path):\n",
    "    \"\"\"\n",
    "    Process a single analysis.json file and extract timeline data.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the analysis.json file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing timeline data\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'steps' not in data:\n",
    "        print(f\"Warning: {file_path} does not contain 'steps' key, skipping\")\n",
    "        return None\n",
    "    \n",
    "    # Process the data\n",
    "    steps = data['steps']\n",
    "    \n",
    "    # Extract run ID from the file path (timestamp directory)\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    run_id = path_parts[-2] if len(path_parts) >= 2 else \"Unknown_Run\"\n",
    "    \n",
    "    # Group steps by stage transitions\n",
    "    transitions = []\n",
    "    current_stage = None\n",
    "    start_step = None\n",
    "    start_time = None\n",
    "    last_step_id = None\n",
    "    last_step_time = None\n",
    "    \n",
    "    for step in steps:\n",
    "        # Skip steps without Stage information\n",
    "        if 'Stage' not in step:\n",
    "            continue\n",
    "            \n",
    "        stage = step['Stage']\n",
    "        step_time = step.get('Timestamp')\n",
    "        \n",
    "        # Keep track of the last step in the current stage\n",
    "        if stage == current_stage:\n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # When stage changes, record the previous stage block\n",
    "        elif current_stage is not None:\n",
    "            # For stages with only a single step:\n",
    "            if start_step == last_step_id:\n",
    "                # If this is not the first step in the entire process\n",
    "                prev_step_time = None\n",
    "                for prev_step in steps:\n",
    "                    if prev_step['Step_ID'] == start_step - 1:\n",
    "                        prev_step_time = prev_step.get('Timestamp')\n",
    "                        break\n",
    "                \n",
    "                # If we found the previous step, use its timestamp as the start time\n",
    "                if prev_step_time is not None and last_step_time is not None:\n",
    "                    duration = last_step_time - prev_step_time\n",
    "                else:\n",
    "                    # Fallback: estimate based on the current step\n",
    "                    duration = 5  # default seconds for a single step with no context\n",
    "            else:\n",
    "                # For multi-step stages, the duration is from the prior stage's last step\n",
    "                # to this stage's last step\n",
    "                duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "            \n",
    "            # Add the previous stage block\n",
    "            transitions.append({\n",
    "                'stage': current_stage,\n",
    "                'start_step': start_step,\n",
    "                'end_step': last_step_id,\n",
    "                'position': len(transitions),\n",
    "                'start_time': start_time,\n",
    "                'end_time': last_step_time,\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            # Start a new stage block\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # Find the timestamp of the previous step to use as start_time\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Use previous step's timestamp as start_time\n",
    "            start_time = prev_step_time\n",
    "            \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # Initialize the first stage\n",
    "        else:\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # For the first step, use a reasonable time before\n",
    "            if step_time is not None:\n",
    "                # Estimate start time as 5 seconds before first timestamp\n",
    "                start_time = step_time - 5  # assuming the first step took 5 seconds\n",
    "            else:\n",
    "                start_time = None\n",
    "                \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "    \n",
    "    # Add the final stage block\n",
    "    if current_stage is not None:\n",
    "        # For single step final stage\n",
    "        if start_step == last_step_id:\n",
    "            # Try to find the previous step timestamp\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Calculate duration from previous step to current\n",
    "            if prev_step_time is not None and last_step_time is not None:\n",
    "                duration = last_step_time - prev_step_time\n",
    "            else:\n",
    "                # If no previous step, estimate a reasonable duration\n",
    "                duration = 5  # default seconds\n",
    "        else:\n",
    "            # For multi-step stages\n",
    "            duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "        \n",
    "        transitions.append({\n",
    "            'stage': current_stage,\n",
    "            'start_step': start_step,\n",
    "            'end_step': last_step_id,\n",
    "            'position': len(transitions),\n",
    "            'start_time': start_time,\n",
    "            'end_time': last_step_time,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    # Count steps in each stage for proportional sizing\n",
    "    for t in transitions:\n",
    "        t['step_count'] = t['end_step'] - t['start_step'] + 1\n",
    "    \n",
    "    # Calculate total duration\n",
    "    total_duration = sum(t['duration'] for t in transitions if t['duration'] is not None)\n",
    "    \n",
    "    return {\n",
    "        'run_id': run_id,\n",
    "        'transitions': transitions,\n",
    "        'total_duration': total_duration\n",
    "    }\n",
    "\n",
    "def draw_timeline(ax, timeline_data, y_pos, height, stage_colors, stage_names, run_id, width_scale):\n",
    "    \"\"\"\n",
    "    Draw a single timeline on the given axes\n",
    "    \n",
    "    Args:\n",
    "        ax: Matplotlib axes to draw on\n",
    "        timeline_data: Dictionary containing timeline data\n",
    "        y_pos: Y position for this timeline\n",
    "        height: Height of the timeline\n",
    "        stage_colors: Dictionary mapping stage numbers to colors\n",
    "        stage_names: Dictionary mapping stage numbers to names\n",
    "        run_id: ID of this run (for labeling)\n",
    "        width_scale: Factor to scale duration to figure width (seconds per unit width)\n",
    "    \"\"\"\n",
    "    transitions = timeline_data['transitions']\n",
    "    total_width = 1.0  # Total width of the figure is 1.0\n",
    "    \n",
    "    # Add run label on the left with more space\n",
    "    ax.text(-0.05, y_pos, run_id, ha='right', va='center', fontsize=9, fontfamily='monospace')\n",
    "    \n",
    "    # Draw timeline background - full width rectangle with light gray\n",
    "    rect_bg = patches.Rectangle(\n",
    "        (0, y_pos - height/2),\n",
    "        total_width,\n",
    "        height,\n",
    "        linewidth=0,\n",
    "        facecolor='#f8f8f8',\n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax.add_patch(rect_bg)\n",
    "    \n",
    "    # Calculate total duration for scaling\n",
    "    total_duration = timeline_data['total_duration']\n",
    "    if total_duration <= 0:\n",
    "        print(f\"Warning: Run {run_id} has no duration information\")\n",
    "        return\n",
    "    \n",
    "    # The blocks will be drawn with width proportional to their duration using the common scale\n",
    "    current_pos = 0\n",
    "    \n",
    "    # Draw the timeline blocks\n",
    "    for i, block in enumerate(transitions):\n",
    "        # Skip blocks with no duration\n",
    "        if block['duration'] is None or block['duration'] <= 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate width using the common scale for all timelines\n",
    "        # This ensures 1 hour on one timeline looks the same as 1 hour on another\n",
    "        block_width = block['duration'] / width_scale\n",
    "        \n",
    "        # Ensure minimum visibility\n",
    "        ### CAN MAKE THIS ADJUSTABLE\n",
    "        min_width = 0.005\n",
    "        if block_width < min_width:\n",
    "            block_width = min_width\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (current_pos, y_pos - height/2),\n",
    "            block_width,\n",
    "            height,\n",
    "            linewidth=0.3,\n",
    "            edgecolor='black',\n",
    "            facecolor=stage_colors.get(block['stage'], \"#cccccc\"),  # Default to gray if stage not in colors\n",
    "            alpha=0.8\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add step count to each block - always try to show the count\n",
    "        mid_point = current_pos + block_width/2\n",
    "        \n",
    "        # For narrow blocks, show number but smaller\n",
    "        font_size = 7\n",
    "        if block_width < 0.05:\n",
    "            font_size = 6\n",
    "        \n",
    "        # Always show the step count\n",
    "        # ax.text(\n",
    "        #     mid_point, y_pos, \n",
    "        #     f\"{block['step_count']}\", \n",
    "        #     ha='center', va='center', \n",
    "        #     fontsize=font_size, \n",
    "        #     fontweight='bold',\n",
    "        #     color='black'\n",
    "        # )\n",
    "        \n",
    "        # Move to next position\n",
    "        current_pos += block_width\n",
    "    \n",
    "    # Add total duration at the end of timeline\n",
    "    if total_duration is not None:\n",
    "        # Format duration\n",
    "        if total_duration >= 3600:\n",
    "            hours = int(total_duration // 3600)\n",
    "            minutes = int((total_duration % 3600) // 60)\n",
    "            duration_str = f\"{hours}h {minutes}m\"\n",
    "        elif total_duration >= 60:\n",
    "            minutes = int(total_duration // 60)\n",
    "            seconds = int(total_duration % 60)\n",
    "            duration_str = f\"{minutes}m {seconds}s\"\n",
    "        else:\n",
    "            duration_str = f\"{int(total_duration)}s\"\n",
    "        \n",
    "        ax.text(1.05, y_pos, duration_str, ha='left', va='center', fontsize=8, fontfamily='monospace')\n",
    "\n",
    "def create_task_timeline(task_name, run_data, output_dir):\n",
    "    \"\"\"\n",
    "    Create a timeline visualization for all runs of a task\n",
    "    \n",
    "    Args:\n",
    "        task_name: Name of the task\n",
    "        run_data: List of dictionaries containing timeline data for each run\n",
    "        output_dir: Directory to save the output figure\n",
    "    \"\"\"\n",
    "    # Define stage names - updated to 7 stages\n",
    "    stage_names = {\n",
    "        1: \"Understanding & Exploration\",\n",
    "        2: \"Baseline Assessment\",\n",
    "        3: \"Problem Analysis & Idea Generation\",\n",
    "        4: \"Implementation\",\n",
    "        5: \"Debugging & Error Handling\",\n",
    "        6: \"Experimental Refinement\",\n",
    "        7: \"Final Evaluation & Submission\"\n",
    "    }\n",
    "    \n",
    "    # Define stage colors - updated to 7 stages with distinct colors\n",
    "    stage_colors = {\n",
    "        1: \"#b3e6b3\",  # Light green\n",
    "        2: \"#b3b3e6\",  # Light blue\n",
    "        3: \"#e6b3b3\",  # Light red\n",
    "        4: \"#e6e6b3\",  # Light yellow\n",
    "        5: \"#e6b3e6\",  # Light purple\n",
    "        6: \"#b3e6e6\",  # Light cyan\n",
    "        7: \"#e6cc99\"   # Light orange\n",
    "    }\n",
    "    \n",
    "    # Sort runs by run_id\n",
    "    sorted_runs = sorted(run_data, key=lambda x: x['run_id'])\n",
    "    \n",
    "    # Calculate a common width scale for all timelines\n",
    "    # Find max duration across all runs\n",
    "    max_duration = max([r['total_duration'] for r in sorted_runs if r['total_duration'] > 0], default=3600)\n",
    "    \n",
    "    # We want to scale so that the timeline with maximum duration fills 90% of the width\n",
    "    # This gives some room for labels\n",
    "    width_scale = max_duration / 0.9  # seconds per unit width\n",
    "    \n",
    "    # Create figure with appropriate dimensions\n",
    "    num_runs = len(sorted_runs)\n",
    "    fig_width = 15  # Wider figure for better readability\n",
    "    fig_height = max(7, 1.5 + 0.3 * num_runs)  # Adjust height based on number of runs\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    # Set up the figure with a white background\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Calculate spacing for timelines\n",
    "    timeline_height = 0.1  # Height of each timeline\n",
    "    y_spacing = 1.0 / (num_runs + 1)  # Equal spacing based on number of runs\n",
    "    \n",
    "    # Draw vertical gridlines first (every 10% of width)\n",
    "    for x in np.arange(0, 1.1, 0.1):\n",
    "        ax.axvline(x=x, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Draw horizontal gridlines - one per run (top and bottom of each row)\n",
    "    for i in range(num_runs):\n",
    "        y = 1.0 - y_spacing * (i + 1)\n",
    "        ax.axhline(y=y - timeline_height/2, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "        ax.axhline(y=y + timeline_height/2, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Draw each timeline using the common scale\n",
    "    for i, run_data in enumerate(sorted_runs):\n",
    "        y_pos = 1.0 - y_spacing * (i + 1)\n",
    "        run_label = f\"Run {i+1}\"\n",
    "        draw_timeline(ax, run_data, y_pos, timeline_height, stage_colors, stage_names, run_label, width_scale)\n",
    "    \n",
    "    # Format task name for display\n",
    "    display_task_name = task_name.replace('_', ' ').title()\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f'{display_task_name} - Stage Timelines Across Runs', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Add a legend for stages\n",
    "    legend_elements = []\n",
    "    for stage in sorted(stage_names.keys()):\n",
    "        legend_elements.append(\n",
    "            patches.Patch(facecolor=stage_colors[stage], \n",
    "                          edgecolor='black', \n",
    "                          label=f'Stage {stage}: {stage_names[stage]}')\n",
    "        )\n",
    "    \n",
    "    # Position legend at the bottom with enough space - adjusted for more stages\n",
    "    legend = plt.legend(handles=legend_elements, loc='upper center', \n",
    "               bbox_to_anchor=(0.5, -0.1 - 0.015 * num_runs), ncol=3,  # Changed from 5 to 3 columns\n",
    "               fancybox=True, shadow=True, fontsize=9)\n",
    "    legend.get_frame().set_linewidth(0.5)\n",
    "    \n",
    "    # Add note about timeline visualization (positioned based on number of runs)\n",
    "    plt.figtext(0.5, 0.01, \n",
    "                \"Note: Block widths are proportional to time duration in each stage.\\nNumbers inside blocks show step count. Total duration shown on the right.\", \n",
    "                ha='center', fontsize=9, \n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='lightgray', boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Set up the axes\n",
    "    ax.set_xlim(-0.15, 1.15)  # More space on sides for labels\n",
    "    ax.set_ylim(0, 1.2)       # More space on top\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add border around the plot\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.5)\n",
    "        spine.set_edgecolor('black')\n",
    "    \n",
    "    # Add more space at the bottom for legend and notes - adjusted for more stages\n",
    "    plt.subplots_adjust(bottom=0.2 + 0.02 * num_runs, left=0.15, right=0.85, top=0.95)\n",
    "    \n",
    "    # Generate output filename\n",
    "    output_filename = os.path.join(output_dir, f\"{task_name}_all_runs_timeline.png\")\n",
    "    \n",
    "    # Save the figure without using tight_layout (which causes warnings)\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Generated timeline for {task_name} at {output_filename}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def find_analysis_files(root_dir):\n",
    "    \"\"\"\n",
    "    Find all analysis.json files under the root directory\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory to start the search\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping task names to lists of file paths\n",
    "    \"\"\"\n",
    "    task_files = defaultdict(list)\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'analysis.json' in filenames:\n",
    "            file_path = os.path.join(dirpath, 'analysis.json')\n",
    "            # Extract task name (Steps/task_name/run_id/analysis.json)\n",
    "            path_parts = file_path.split(os.sep)\n",
    "            if len(path_parts) >= 3 and path_parts[0] == root_dir:\n",
    "                task_name = path_parts[1]\n",
    "                task_files[task_name].append(file_path)\n",
    "    \n",
    "    return task_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 analysis.json files across 7 tasks\n",
      "Processing task: erasing_invisible_watermarks with 8 runs\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/erasing_invisible_watermarks_all_runs_timeline.png\n",
      "Processing task: product-recommendation with 8 runs\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/product-recommendation_all_runs_timeline.png\n",
      "Processing task: machine_unlearning with 8 runs\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/machine_unlearning_all_runs_timeline.png\n",
      "Processing task: meta-learning with 8 runs\n",
      "Generated timeline for meta-learning at Steps/meta-learning/meta-learning_all_runs_timeline.png\n",
      "Processing task: llm-merging with 8 runs\n",
      "Generated timeline for llm-merging at Steps/llm-merging/llm-merging_all_runs_timeline.png\n",
      "Processing task: backdoor-trigger-recovery with 8 runs\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/backdoor-trigger-recovery_all_runs_timeline.png\n",
      "Processing task: perception_temporal_action_loc with 8 runs\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/perception_temporal_action_loc_all_runs_timeline.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Root directory to search for analysis.json files\n",
    "root_dir = 'Steps'\n",
    "\n",
    "# Find all analysis.json files grouped by task\n",
    "task_files = find_analysis_files(root_dir)\n",
    "\n",
    "print(f\"Found {sum(len(files) for files in task_files.values())} analysis.json files across {len(task_files)} tasks\")\n",
    "\n",
    "# Process each task\n",
    "for task_name, file_paths in task_files.items():\n",
    "    print(f\"Processing task: {task_name} with {len(file_paths)} runs\")\n",
    "    \n",
    "    # Process each run for this task\n",
    "    run_data = []\n",
    "    for file_path in file_paths:\n",
    "        timeline_data = process_analysis_json(file_path)\n",
    "        if timeline_data:\n",
    "            run_data.append(timeline_data)\n",
    "    \n",
    "    # Generate a consolidated timeline for this task's runs\n",
    "    if run_data:\n",
    "        output_dir = os.path.join(root_dir, task_name)\n",
    "        create_task_timeline(task_name, run_data, output_dir)\n",
    "    else:\n",
    "        print(f\"No valid data found for task: {task_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_timeline_grid(task_data, output_dir, output_filename=\"combined_timelines.pdf\", custom_titles=None):\n",
    "    \"\"\"\n",
    "    Create a grid of timeline visualizations for multiple tasks with a shared legend\n",
    "    \n",
    "    Args:\n",
    "        task_data: Dictionary mapping task names to lists of run data\n",
    "        output_dir: Directory to save the output figure\n",
    "        output_filename: Name of the output file\n",
    "    \"\"\"\n",
    "    # Define stage names and colors (same as in create_task_timeline)\n",
    "    stage_names = {\n",
    "        1: \"Understanding & Exploration\",\n",
    "        2: \"Baseline Assessment\",\n",
    "        3: \"Problem Analysis & Idea Generation\",\n",
    "        4: \"Implementation\",\n",
    "        5: \"Debugging & Error Handling\",\n",
    "        6: \"Experimental Refinement\",\n",
    "        7: \"Final Evaluation & Submission\"\n",
    "    }\n",
    "    \n",
    "    stage_colors = {\n",
    "        1: \"#b3e6b3\",  # Light green\n",
    "        2: \"#b3b3e6\",  # Light blue\n",
    "        3: \"#e6b3b3\",  # Light red\n",
    "        4: \"#e6e6b3\",  # Light yellow\n",
    "        5: \"#e6b3e6\",  # Light purple\n",
    "        6: \"#b3e6e6\",  # Light cyan\n",
    "        7: \"#e6cc99\"   # Light orange\n",
    "    }\n",
    "    \n",
    "    # Create a 4x2 grid figure (3x2 for tasks, bottom row for legend)\n",
    "    fig = plt.figure(figsize=(24, 24))  # Wider and taller for better readability\n",
    "    \n",
    "    # Create GridSpec with 4 rows, 2 columns\n",
    "    # The last row is shorter than the others for the legend\n",
    "    gs = plt.GridSpec(5, 2, height_ratios=[1, 1, 1, 1, 0.4])\n",
    "    \n",
    "    # Get list of tasks (limit to 6 for the 3x2 grid)\n",
    "    task_names = list(task_data.keys())[:7]\n",
    "    \n",
    "    # If we have fewer than 6 tasks, adjust accordingly\n",
    "    num_tasks = len(task_names)\n",
    "    \n",
    "    # Calculate a common width scale for all timelines across all tasks\n",
    "    # Find max duration across all runs in all tasks\n",
    "    max_duration = 0\n",
    "    for task_name in task_names:\n",
    "        for run_data in task_data[task_name]:\n",
    "            if run_data['total_duration'] > max_duration:\n",
    "                max_duration = run_data['total_duration']\n",
    "    \n",
    "    # Default to 1 hour if no valid durations\n",
    "    if max_duration <= 0:\n",
    "        max_duration = 3600\n",
    "    \n",
    "    # Scale so that the timeline with maximum duration fills 90% of the width\n",
    "    width_scale = max_duration / 0.9  # seconds per unit width\n",
    "    \n",
    "    # Create each task subplot\n",
    "    for i, task_name in enumerate(task_names):\n",
    "        # Special handling for the 7th task\n",
    "        if i == 6:\n",
    "            # Create a subplot that's centered in the bottom row\n",
    "            # We'll create it with the same width as other tasks\n",
    "            ax = fig.add_subplot(gs[3, :])  # First get full width\n",
    "            \n",
    "            # Calculate the position for a centered subplot with width of one column\n",
    "            # This keeps same width as other subplots but centers it horizontally\n",
    "            box = ax.get_position()\n",
    "            width = box.width / 2  # Half the full width (same as one column)\n",
    "            center = box.x0 + box.width / 2  # Center point of the full width\n",
    "            \n",
    "            # Set new position: centered with width of one column\n",
    "            ax.set_position([center - width/2, box.y0, width, box.height])\n",
    "            \n",
    "            # Clear the full-width axis we don't need\n",
    "            ax.cla()\n",
    "        else:\n",
    "            # For first 6 tasks, use standard 3x2 grid\n",
    "            row = i // 2\n",
    "            col = i % 2\n",
    "            ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        # Get run data for this task\n",
    "        run_data_list = task_data[task_name]\n",
    "        sorted_runs = sorted(run_data_list, key=lambda x: x['run_id'])\n",
    "        num_runs = len(sorted_runs)\n",
    "        \n",
    "        # Set up the subplot with a white background\n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        # Calculate spacing for timelines\n",
    "        timeline_height = 0.1  # Height of each timeline\n",
    "        y_spacing = 1.0 / (num_runs + 1)  # Equal spacing based on number of runs\n",
    "        \n",
    "        # Draw vertical gridlines (every 10% of width)\n",
    "        for x in np.arange(0, 1.1, 0.1):\n",
    "            ax.axvline(x=x, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "        \n",
    "        # Draw horizontal gridlines - one per run\n",
    "        for j in range(num_runs):\n",
    "            y = 1.0 - y_spacing * (j + 1)\n",
    "            ax.axhline(y=y - timeline_height/2, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "            ax.axhline(y=y + timeline_height/2, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "        \n",
    "        # Draw each timeline using the common scale\n",
    "        for j, run_data in enumerate(sorted_runs):\n",
    "            y_pos = 1.0 - y_spacing * (j + 1)\n",
    "            run_label = f\"Run {j+1}\"\n",
    "            draw_timeline(ax, run_data, y_pos, timeline_height, stage_colors, stage_names, run_label, width_scale)\n",
    "        \n",
    "        # Use custom title if provided, otherwise format the task name\n",
    "        if custom_titles and task_name in custom_titles:\n",
    "            display_task_name = custom_titles[task_name]\n",
    "        else:\n",
    "            display_task_name = task_name.replace('_', ' ').title()\n",
    "        \n",
    "        # Add title for this subplot\n",
    "        ax.set_title(f'{display_task_name}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Set up the axes\n",
    "        ax.set_xlim(-0.15, 1.15)  # More space on sides for labels\n",
    "        ax.set_ylim(0, 1.2)       # More space on top\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Add border around the plot\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(1.0)\n",
    "            spine.set_edgecolor('black')\n",
    "    \n",
    "    # Create a shared legend in the bottom row\n",
    "    legend_ax = fig.add_subplot(gs[4, :])\n",
    "    legend_ax.axis('off')\n",
    "    \n",
    "    # Add a legend for stages\n",
    "    legend_elements = []\n",
    "    for stage in sorted(stage_names.keys()):\n",
    "        legend_elements.append(\n",
    "            patches.Patch(facecolor=stage_colors[stage], \n",
    "                          edgecolor='black', \n",
    "                          label=f'Stage {stage}: {stage_names[stage]}')\n",
    "        )\n",
    "    \n",
    "    # Create the legend - position it higher in the row\n",
    "    legend = legend_ax.legend(handles=legend_elements, loc='upper center', \n",
    "                             bbox_to_anchor=(0.5, 0.9), ncol=4,\n",
    "                             fancybox=True, shadow=True, fontsize=12)\n",
    "    legend.get_frame().set_linewidth(0.5)\n",
    "    \n",
    "    # Add note about timeline visualization - moved lower to avoid overlap\n",
    "    legend_ax.text(0.5, 0.2, \n",
    "                  \"Note: Block widths are proportional to time duration in each stage.\\n\"\n",
    "                  \"Total duration for each run is shown on the right of each timeline.\",\n",
    "                  ha='center', fontsize=12, \n",
    "                  bbox=dict(facecolor='white', alpha=0.8, edgecolor='lightgray', boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle('Stage Timelines Across Multiple Tasks', fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate output path\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Save the figure\n",
    "    #plt.show()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Generated combined timeline grid at {output_path}\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 analysis.json files across 7 tasks\n",
      "Processing task: erasing_invisible_watermarks with 8 runs\n",
      "Generated timeline for erasing_invisible_watermarks at Steps/erasing_invisible_watermarks/erasing_invisible_watermarks_all_runs_timeline.png\n",
      "Processing task: product-recommendation with 8 runs\n",
      "Generated timeline for product-recommendation at Steps/product-recommendation/product-recommendation_all_runs_timeline.png\n",
      "Processing task: machine_unlearning with 8 runs\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/machine_unlearning_all_runs_timeline.png\n",
      "Processing task: meta-learning with 8 runs\n",
      "Generated timeline for meta-learning at Steps/meta-learning/meta-learning_all_runs_timeline.png\n",
      "Processing task: llm-merging with 8 runs\n",
      "Generated timeline for llm-merging at Steps/llm-merging/llm-merging_all_runs_timeline.png\n",
      "Processing task: backdoor-trigger-recovery with 8 runs\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/backdoor-trigger-recovery_all_runs_timeline.png\n",
      "Processing task: perception_temporal_action_loc with 8 runs\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/perception_temporal_action_loc_all_runs_timeline.png\n",
      "Generated combined timeline grid at Steps/combined_task_timelines.pdf\n"
     ]
    }
   ],
   "source": [
    "custom_titles = {\n",
    "    \"machine_unlearning\": \"Machine Unlearning\",\n",
    "    \"llm-merging\": \"LLM Merging\",\n",
    "    \"backdoor-trigger-recovery\": \"Backdoor Trigger Recovery\",\n",
    "    \"perception_temporal_action_loc\": \"Perception Temporal Action Localization\",\n",
    "    \"product-recommendation\": \"Product Recommendation\",\n",
    "    \"meta-learning\": \"Meta Learning\",\n",
    "    \"erasing_invisible_watermarks\": \"Erasing Invisible Watermarks\"\n",
    "    #\"weather_forcast\": \"Weather Forecast\"\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# After processing all tasks in your main code\n",
    "# Collect all task data in a dictionary\n",
    "all_task_data = {}\n",
    "\n",
    "# Root directory to search for analysis.json files\n",
    "root_dir = 'Steps'\n",
    "\n",
    "# Find all analysis.json files grouped by task\n",
    "task_files = find_analysis_files(root_dir)\n",
    "\n",
    "print(f\"Found {sum(len(files) for files in task_files.values())} analysis.json files across {len(task_files)} tasks\")\n",
    "\n",
    "# Process each task\n",
    "for task_name, file_paths in task_files.items():\n",
    "    print(f\"Processing task: {task_name} with {len(file_paths)} runs\")\n",
    "    \n",
    "    # Process each run for this task\n",
    "    run_data = []\n",
    "    for file_path in file_paths:\n",
    "        timeline_data = process_analysis_json(file_path)\n",
    "        if timeline_data:\n",
    "            run_data.append(timeline_data)\n",
    "    \n",
    "    # Generate a consolidated timeline for this task's runs\n",
    "    if run_data:\n",
    "        output_dir = os.path.join(root_dir, task_name)\n",
    "        create_task_timeline(task_name, run_data, output_dir)\n",
    "        \n",
    "        # Store the run data for the combined grid\n",
    "        all_task_data[task_name] = run_data\n",
    "    else:\n",
    "        print(f\"No valid data found for task: {task_name}\")\n",
    "\n",
    "# Create the combined grid visualization\n",
    "if all_task_data:\n",
    "    create_combined_timeline_grid(all_task_data, root_dir, \"combined_task_timelines.pdf\", custom_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
